import json
import altair as alt
from typing import Dict, Union, Any, List, Tuple
import pandas as pd
import io
import json
import traceback
import ast
from contextlib import redirect_stdout, redirect_stderr
import base64
import httpx
import openai
from multiprocessing import Process, Queue
from tqdm import tqdm
from datetime import datetime
import os
import time
from concurrent.futures import ProcessPoolExecutor
import concurrent.futures
try:
    from PIL import Image
    HAS_PIL = True
except ImportError:
    HAS_PIL = False

# ======================================== Config ========================================

MODEL = "gemini-2.0-flash"
NEED_LOG = True  # 启用日志以便调试图片大小和API调用问题

# 配置日志
log_dir = "./metric/logs"
os.makedirs(log_dir, exist_ok=True)
log_folder = os.path.join(log_dir, f"metric_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log")

def _log(message: str):
    if NEED_LOG:
        with open(log_folder, "a", encoding="utf-8") as f:
            f.write(f"{message}\n")

# ======================================== Prompt ========================================

HIGH_LEVEL_PROMPT = """You are an expert visualization evaluator tasked with comparing charts. The first image (reference image) is created using ground truth altair code, and the second image (AI-generated image) is created using altair code generated by an AI assistant. Score how well the AI-generated plot matches the ground truth plot.

### Scoring Methodology:
The evaluation is based on 6 criteria, with a total possible score of 100 points:

1. **Chart Types (20 points)** - Evaluate if the AI-generated image uses all the same chart types as the reference (e.g., bar charts, line charts, area charts, scatter plots, etc.)
   - 20: Perfect match of all chart types
   - 15: Minor differences in secondary chart elements
   - 10: Some chart types missing or incorrectly implemented
   - 0: Completely different chart types

2. **Layout (10 points)** - Evaluate if the AI-generated image matches the reference's arrangement
   - 10: Perfect match of layout (rows, columns, facets, etc.)
   - 7: Minor differences in arrangement but same structure
   - 4: Some structural layout differences
   - 0: Completely different layout

3. **Text Content (20 points)** - Evaluate if the AI-generated image includes text with equivalent meaning as the reference
   - 20: All text elements convey the same meaning as the reference (titles, subtitles, axis labels, annotations, legends)
   - 15: Minor semantic differences or formatting issues in some text elements
   - 10: Some text elements missing or conveying different meaning than reference
   - 0: Most text elements missing or conveying incorrect meaning

4. **Data (20 points)** - Evaluate if the AI-generated image accurately represents the same data
   - 20: Data visualization appears identical
   - 15: Minor differences in data representation
   - 10: Some data trends or groups misrepresented
   - 0: Data appears completely different

5. **Style (20 points)** - Evaluate if the AI-generated image matches reference styling
   - 20: Perfect match of colors, markers, line styles, and other visual elements
   - 15: Minor styling differences
   - 10: Notable styling differences but similar theme
   - 0: Completely different styling

6. **Clarity (10 points)** - Evaluate if the AI-generated image is clear and readable
   - 10: Perfectly clear, no overlapping elements or readability issues
   - 7: Minor clarity issues
   - 4: Some overlapping or readability problems
   - 0: Major clarity problems

### Required Output Format:
Your evaluation must strictly follow this JSON structure:

```json
{
  "chart_types": {
    "score": <score_value>,
    "max": 20,
    "comment": "<brief_justification>"
  },
  "layout": {
    "score": <score_value>,
    "max": 10,
    "comment": "<brief_justification>"
  },
  "text_content": {
    "score": <score_value>,
    "max": 20,
    "comment": "<brief_justification>"
  },
  "data": {
    "score": <score_value>,
    "max": 20,
    "comment": "<brief_justification>"
  },
  "style": {
    "score": <score_value>,
    "max": 20,
    "comment": "<brief_justification>"
  },
  "clarity": {
    "score": <score_value>,
    "max": 10,
    "comment": "<brief_justification>"
  },
  "overall": {
    "score": <sum_of_all_scores>,
    "max": 100,
    "comment": "<overall_assessment>"
  }
}
```

Make sure your response is only valid JSON that can be parsed. Do not include any text outside the JSON structure.
"""

# ======================================== Function ========================================

def call_llm(messages: list, max_retries: int = 10, timeout: int = 120):
    """
    调用LLM API并返回响应
    
    参数:
        messages: 消息列表
        max_retries: 最大重试次数（默认10次）
        timeout: 单次API调用超时时间（秒，默认120秒）
    
    返回:
        LLM响应的文本内容
    """
    retry_count = 0
    last_error = None
    
    while retry_count < max_retries:
        http_client = None
        try:
            # 每次重试都创建新的客户端，避免连接状态问题
            http_client = httpx.Client(verify=False, timeout=timeout)
            
            # 创建客户端时添加SSL相关配置
            if "gpt" in MODEL:
                client = openai.Client(
                    api_key= "xxx",
                    base_url= "xxx",
                    http_client=http_client
                )
            elif "qwen" in MODEL:
                client = openai.Client(
                    api_key= "xxx", 
                    base_url= "xxx",
                    http_client=http_client
                )
            elif "gemini" in MODEL:
                client = openai.Client(
                    api_key= "xxx",
                    base_url= "xxx",
                    http_client=http_client
                )
            else:
                raise ValueError(f"Unsupported model: {MODEL}")
            
            _log(f"Calling LLM (attempt {retry_count + 1}/{max_retries})...")
            
            response = client.chat.completions.create(
                model=MODEL,
                messages=messages,
                temperature=0.0,
                timeout=timeout
            )
            
            _log(f"LLM call successful")
            return response.choices[0].message.content
            
        except Exception as e:
            last_error = e
            retry_count += 1
            error_msg = f"Error calling LLM (attempt {retry_count}/{max_retries}): {str(e)}"
            _log(error_msg)
            print(error_msg)  # 同时打印到控制台，方便调试
            
            # 检查是否是无效参数错误（通常是图片太大）
            error_str = str(e).lower()
            if "400" in error_str and ("invalid_argument" in error_str or "invalid argument" in error_str):
                error_msg = "ERROR: Invalid argument (likely image too large or invalid format). Stopping retries."
                _log(error_msg)
                print(error_msg)
                raise Exception("Invalid argument error - image may be too large or in wrong format")
            
            if retry_count < max_retries:
                wait_time = min(2 ** retry_count, 30)  # 指数退避，最多等待30秒
                _log(f"Waiting {wait_time} seconds before retry...")
                time.sleep(wait_time)
        finally:
            # 确保关闭http_client
            if http_client is not None:
                try:
                    http_client.close()
                except:
                    pass
    
    # 达到最大重试次数后仍然失败
    error_msg = f"Failed to call LLM after {max_retries} attempts. Last error: {str(last_error)}"
    _log(error_msg)
    print(error_msg)
    raise Exception(error_msg)

# 在子进程中执行Altair代码并通过队列返回结果
def exec_altair_code_in_process(code_string, queue):
    """
    Execute Altair code in a separate process to ensure counter consistency.
    """
    try:
        # 基本设置
        result = {
            'success': False,
            'chart': None,
            'output': '',
            'error': None
        }
        
        # 告知主进程开始执行
        queue.put({"status": "running"})
        
        # 初始化 original_renderer 为默认值
        original_renderer = alt.renderers.active
        
        # Capture stdout and stderr
        stdout_capture = io.StringIO()
        stderr_capture = io.StringIO()
        
        # Create a namespace with commonly needed libraries
        namespace = {
            'alt': alt,
            'pd': pd
        }
        
        try:
            last_expr_value = None
            lines = code_string.split("\n")
            lines_new = [line for line in lines if "exit(" not in line and ".to_json()" not in line and "print(" not in line]
            modified_code = '\n'.join(lines_new).replace("exit()", "")
            modified_code = modified_code.replace(".show()", "")
            modified_code = modified_code.replace(".display(", "# .display(")
            original_renderer = alt.renderers.active  # 重新赋值
            alt.renderers.enable('default')
            
            try:
                parsed_code = ast.parse(modified_code)
                if parsed_code.body and isinstance(parsed_code.body[-1], ast.Expr):
                    last_expr = ast.unparse(parsed_code.body[-1])
                    code_without_last = ast.unparse(ast.Module(body=parsed_code.body[:-1], type_ignores=[]))
                    with redirect_stdout(stdout_capture), redirect_stderr(stderr_capture):
                        exec(code_without_last, namespace)
                    with redirect_stdout(stdout_capture), redirect_stderr(stderr_capture):
                        last_expr_value = eval(last_expr, namespace)
                else:
                    with redirect_stdout(stdout_capture), redirect_stderr(stderr_capture):
                        exec(modified_code, namespace)
            except SyntaxError:
                with redirect_stdout(stdout_capture), redirect_stderr(stderr_capture):
                    exec(modified_code, namespace)
            
            chart = None
            if isinstance(last_expr_value, alt.TopLevelMixin):
                chart = last_expr_value
            else:
                if 'chart' in namespace and isinstance(namespace['chart'], alt.TopLevelMixin):
                    chart = namespace['chart']
                else:
                    # Otherwise look for charts in variables, prioritizing more complex charts
                    chart_candidates = []
                    for var_name, var_value in namespace.items():
                        if isinstance(var_value, alt.TopLevelMixin):
                            chart_candidates.append((var_name, var_value))
                    
                    if chart_candidates:
                        if len(chart_candidates) > 1:
                            final_charts = [c for _, c in chart_candidates if hasattr(c, 'title') and c.title is not None]
                            if final_charts:
                                chart = final_charts[0]
                            else:
                                chart = chart_candidates[-1][1]
                        else:
                            chart = chart_candidates[0][1]
            
            result['success'] = True
            result['chart'] = chart
            result['output'] = stdout_capture.getvalue()
            
        except Exception as e:
            result['success'] = False
            result['error'] = f"{type(e).__name__}: {str(e)}\n{traceback.format_exc()}"
            result['output'] = stdout_capture.getvalue() + stderr_capture.getvalue()
        finally:
            # 确保 original_renderer 已定义
            if 'original_renderer' in locals():
                alt.renderers.enable(original_renderer)
        
        # 最后返回结果
        queue.put(result)
    except Exception as e:
        # 确保任何异常都被捕获并通过队列返回
        error_result = {
            'success': False,
            'chart': None,
            'output': '',
            'error': f"Process exception: {type(e).__name__}: {str(e)}\n{traceback.format_exc()}"
        }
        queue.put(error_result)


def exec_altair_code(code_string, timeout=60):
    """
    Execute a Python code string that uses the Altair library and return the result.
    Attempts to use multiprocessing, but falls back to direct execution if multiprocessing fails.
    
    Parameters:
    -----------
    code_string : str
        A string containing Python code with Altair visualization code
    timeout : int
        Timeout in seconds (default 60)
        
    Returns:
    --------
    dict
        A dictionary containing:
        - 'success': boolean indicating if execution was successful
        - 'chart': The Altair chart object if successful, None otherwise
        - 'output': Any stdout output captured during execution
        - 'error': Error message if execution failed, None otherwise
    """
    # Check if code is accessing database
    is_db_operation = 'sqlite3' in code_string or 'connect(' in code_string
    
    # If direct execution would be simpler (no threading issues)
    if is_db_operation and 'jupyter' not in code_string:
        _log("Detected database operation, using direct execution")
        return _exec_altair_direct(code_string)
    
    try:
        # Try to use multiprocessing for isolation
        queue = Queue()
        
        # Execute the code in a separate process
        process = Process(target=exec_altair_code_in_process, args=(code_string, queue))
        process.start()
        
        # Wait for process to start running
        start_time = time.time()
        process_started = False
        
        # Wait up to 10 seconds for process to start
        while time.time() - start_time < 10:
            if not queue.empty():
                status = queue.get()
                if status.get("status") == "running":
                    process_started = True
                    break
            time.sleep(0.1)
        
        if not process_started:
            _log("Process failed to start in time, falling back to direct execution")
            process.terminate()
            process.join()
            return _exec_altair_direct(code_string)
        
        # Set a timeout in case the process hangs
        process.join(timeout=timeout)
        
        # Check if the process is still alive (timed out)
        if process.is_alive():
            process.terminate()
            process.join()
            
            # If the process timed out, try direct execution as fallback
            _log(f"Execution timed out after {timeout} seconds, trying direct execution")
            return _exec_altair_direct(code_string)
        else:
            # Get the result from the queue if process completed
            try:
                if not queue.empty():
                    result = queue.get(block=False)
                    return result
                else:
                    _log("Queue is empty but process ended, falling back to direct execution")
                    return _exec_altair_direct(code_string)
            except Exception as e:
                _log(f"Failed to retrieve result from process: {str(e)}")
                return _exec_altair_direct(code_string)
        
    except Exception as e:
        _log(f"Multiprocessing failed, falling back to direct execution: {str(e)}")
        return _exec_altair_direct(code_string)


def _exec_altair_direct(code_string):
    """
    Execute Altair code directly without multiprocessing.
    """
    result = {
        'success': False,
        'chart': None,
        'output': '',
        'error': None
    }
    
    stdout_capture = io.StringIO()
    stderr_capture = io.StringIO()
    
    namespace = {
        'alt': alt,
        'pd': pd
    }
    
    try:
        last_expr_value = None
        lines = code_string.split("\n")
        lines_new = [line for line in lines if "exit(" not in line and ".to_json()" not in line and "print(" not in line]
        modified_code = '\n'.join(lines_new).replace("exit()", "")
        modified_code = modified_code.replace(".show()", "")
        modified_code = modified_code.replace(".display(", "# .display(")
        original_renderer = alt.renderers.active
        alt.renderers.enable('default')
        
        try:
            parsed_code = ast.parse(modified_code)
            if parsed_code.body and isinstance(parsed_code.body[-1], ast.Expr):
                last_expr = ast.unparse(parsed_code.body[-1])
                code_without_last = ast.unparse(ast.Module(body=parsed_code.body[:-1], type_ignores=[]))
                with redirect_stdout(stdout_capture), redirect_stderr(stderr_capture):
                    exec(code_without_last, namespace)
                with redirect_stdout(stdout_capture), redirect_stderr(stderr_capture):
                    last_expr_value = eval(last_expr, namespace)
            else:
                with redirect_stdout(stdout_capture), redirect_stderr(stderr_capture):
                    exec(modified_code, namespace)
        except SyntaxError:
            with redirect_stdout(stdout_capture), redirect_stderr(stderr_capture):
                exec(modified_code, namespace)
        
        chart = None
        if isinstance(last_expr_value, alt.TopLevelMixin):
            chart = last_expr_value
        else:
            if 'chart' in namespace and isinstance(namespace['chart'], alt.TopLevelMixin):
                chart = namespace['chart']
            else:
                # Otherwise look for charts in variables, prioritizing more complex charts
                chart_candidates = []
                for var_name, var_value in namespace.items():
                    if isinstance(var_value, alt.TopLevelMixin):
                        chart_candidates.append((var_name, var_value))
                
                if chart_candidates:
                    if len(chart_candidates) > 1:
                        final_charts = [c for _, c in chart_candidates if hasattr(c, 'title') and c.title is not None]
                        if final_charts:
                            chart = final_charts[0]
                        else:
                            chart = chart_candidates[-1][1]
                    else:
                        chart = chart_candidates[0][1]
        
        result['success'] = True
        result['chart'] = chart
        result['output'] = stdout_capture.getvalue()
        
    except Exception as e:
        result['success'] = False
        result['error'] = f"{type(e).__name__}: {str(e)}\n{traceback.format_exc()}"
        result['output'] = stdout_capture.getvalue() + stderr_capture.getvalue()
    finally:
        alt.renderers.enable(original_renderer)
    
    return result

def chart_to_img_url(chart: alt.Chart, format: str = 'png', max_size_mb: float = 5.0, scale_factor: float = 1.0, quality: int = 85) -> Tuple[bool, str]:
    """
    将Altair图表转换为base64编码的字符串，自动压缩过大的图片
    
    Args:
        chart: Altair图表对象
        format: 输出格式，默认为'png'
        max_size_mb: 最大允许的图片大小（MB），默认5MB
        scale_factor: 图片缩放因子，默认1.0（不缩放）
        quality: JPEG压缩质量（1-100），默认85
        
    Returns:
        Tuple[bool, str]: (成功标志, base64编码的图片URL或None)
    """
    try:
        # 将图表转换为指定格式
        buffer = io.BytesIO()
        
        # 如果需要缩放，先调整图表大小
        if scale_factor != 1.0:
            # 获取原始宽度和高度
            original_width = chart.width if hasattr(chart, 'width') and chart.width else 400
            original_height = chart.height if hasattr(chart, 'height') and chart.height else 300
            
            # 创建缩放后的图表
            chart = chart.properties(
                width=int(original_width * scale_factor),
                height=int(original_height * scale_factor)
            )
        
        # 保存图表（某些格式不支持ppi参数）
        try:
            chart.save(buffer, format=format, ppi=72)
        except TypeError:
            chart.save(buffer, format=format)
        buffer.seek(0)
        img_data = buffer.getvalue()
        
        # 检查图片大小
        size_mb = len(img_data) / (1024 * 1024)
        _log(f"Chart image size: {size_mb:.2f} MB (scale: {scale_factor}, quality: {quality})")
        
        # 如果图片太大，尝试使用PIL压缩或降低质量
        if size_mb > max_size_mb:
            if HAS_PIL and format == 'png':
                _log(f"Image too large ({size_mb:.2f} MB), converting to JPEG with quality {quality}")
                try:
                    # 使用PIL将PNG转换为JPEG以减小文件大小
                    img = Image.open(io.BytesIO(img_data))
                    
                    # 如果图片有透明通道，转换为RGB
                    if img.mode in ('RGBA', 'LA', 'P'):
                        # 创建白色背景
                        background = Image.new('RGB', img.size, (255, 255, 255))
                        if img.mode == 'P':
                            img = img.convert('RGBA')
                        background.paste(img, mask=img.split()[-1] if img.mode in ('RGBA', 'LA') else None)
                        img = background
                    
                    # 保存为JPEG
                    jpeg_buffer = io.BytesIO()
                    img.save(jpeg_buffer, format='JPEG', quality=quality, optimize=True)
                    jpeg_buffer.seek(0)
                    img_data = jpeg_buffer.getvalue()
                    
                    size_mb = len(img_data) / (1024 * 1024)
                    _log(f"After JPEG conversion: {size_mb:.2f} MB")
                    
                    if size_mb <= max_size_mb:
                        # 成功压缩到目标大小
                        base64_encoded = base64.b64encode(img_data).decode('utf-8')
                        return True, f"data:image/jpeg;base64,{base64_encoded}"
                    elif quality > 50:
                        # 降低质量再试一次
                        return chart_to_img_url(chart, format=format, max_size_mb=max_size_mb, 
                                              scale_factor=scale_factor, quality=quality - 20)
                except Exception as e:
                    _log(f"PIL compression failed: {str(e)}")
            
            # 如果PIL不可用或压缩失败，尝试缩放
            if scale_factor >= 0.3:
                _log(f"Image too large ({size_mb:.2f} MB), reducing scale to {scale_factor * 0.7:.2f}")
                return chart_to_img_url(chart, format=format, max_size_mb=max_size_mb, 
                                      scale_factor=scale_factor * 0.7, quality=quality)
            else:
                _log(f"ERROR: Image still too large after all attempts ({size_mb:.2f} MB)")
                return False, None
        
        # 编码为base64
        base64_encoded = base64.b64encode(img_data).decode('utf-8')
        
        # 添加对应格式的mime type前缀
        mime_types = {
            'png': 'image/png',
            'svg': 'image/svg+xml',
            'html': 'text/html',
            'jpeg': 'image/jpeg',
            'jpg': 'image/jpeg'
        }
        mime_type = mime_types.get(format, f'image/{format}')
        return True, f"data:{mime_type};base64,{base64_encoded}"
    except Exception as e:
        _log(f"ERROR in chart_to_img_url: {str(e)}\n{traceback.format_exc()}")
        return False, None

# ======================================== Low-Level Metrics ========================================

class LowLevelMetrics:
    """
    Low-Level Metrics for evaluating visualizations based on their structural representation.
    """
    
    @staticmethod
    def _sanitize_dict(obj):
        """
        递归移除字典中的'field'和'as'键，以忽略重命名的差异。
        
        参数：
            obj: 要处理的对象（字典、列表或基本类型）
            
        返回：
            处理后的对象，其中移除了指定的键
        """
        if isinstance(obj, dict):
            # 包含更多应该在比较中被忽略的键
            ignored_keys = ["field", "as", "title", "axis", "scale", "name", "label"]
            return {k: LowLevelMetrics._sanitize_dict(v) for k, v in obj.items() if k not in ignored_keys}
        elif isinstance(obj, list):
            return [LowLevelMetrics._sanitize_dict(item) for item in obj]
        else:
            return obj

    @staticmethod
    def _sanitize_encoding(encoding_obj):
        """
        清理编码对象以进行比较，专注于基本的结构方面。
        移除标题、坐标轴配置和其他表面差异等属性。
        完全忽略字段名称，仅关注编码结构和数据类型。
        
        参数：
            encoding_obj: Altair编码对象
            
        返回：
            清理后的编码对象，用于结构化比较
        """
        if encoding_obj is None:
            return None
            
        # 对于非字典类型的编码，使用通用的清理方法
        if not isinstance(encoding_obj, dict):
            return LowLevelMetrics._sanitize_dict(encoding_obj)
            
        # 对于编码字典，仅提取通道类型和基本数据类型
        result = {}
        for channel, config in encoding_obj.items():
            if isinstance(config, dict):
                # 提取基本类型信息（nominal, quantitative等）
                data_type = None
                if "type" in config:
                    data_type = config["type"]
                elif "shorthand" in config and ":" in config["shorthand"]:
                    # 从简写中提取类型，如"field:O"
                    data_type = config["shorthand"].split(":")[-1]
                
                # 只保留类型信息，完全忽略字段名称
                if data_type:
                    result[channel] = {"type": data_type}
                else:
                    # 如果没有类型信息，就包含一个空字典来表示该通道存在
                    result[channel] = {}
                    
                # 保留聚合函数（如果存在）
                if "aggregate" in config:
                    result[channel]["aggregate"] = config["aggregate"]
                    
                # 如果有bin属性，保留其存在性但不保留具体参数
                if "bin" in config:
                    result[channel]["bin"] = True if config["bin"] else False
            else:
                # 对于简单值，保持原样
                result[channel] = config
                
        return result

    @staticmethod
    def evaluate_chart_type_match(generated_chart: alt.Chart, reference_chart: alt.Chart) -> float:
        """
        评估生成图表与参考图表之间的图表类型匹配程度。
        只比较标记类型，忽略样式属性如颜色、大小、透明度等。
        结构完全匹配返回1.0，否则返回0.0。
        
        参数:
            generated_chart: AI生成的Altair图表
            reference_chart: 参考Altair图表
            
        返回:
            结构完全匹配时返回1.0，否则返回0.0
        """
        try:
            def extract_mark_type(chart: alt.Chart) -> Union[str, List[str]]:
                """
                从图表中提取标记类型。
                
                参数:
                    chart: Altair图表对象
                    
                返回:
                    标记类型（字符串）或图层中的标记类型列表
                """
                try:
                    if hasattr(chart, 'mark') and type(chart.mark) is not alt.utils.schemapi.UndefinedType:
                        # 如果mark是字典或具有type属性，只提取类型
                        if hasattr(chart.mark, 'type'):
                            return chart.mark.type
                        elif isinstance(chart.mark, dict) and 'type' in chart.mark:
                            return chart.mark['type']
                        else:
                            return chart.mark
                    elif hasattr(chart, 'layer') and chart.layer:
                        # 如果是图层图表，提取每个图层的标记类型
                        return [extract_mark_type(layer) for layer in chart.layer]
                    return None
                except Exception as e:
                    _log(f"Error extracting mark type: {str(e)}")
                    return None
            
            gen_mark = extract_mark_type(generated_chart)
            ref_mark = extract_mark_type(reference_chart)
            
            # 如果任一标记提取失败，返回0
            if gen_mark is None or ref_mark is None:
                return 0.0
                
            # 简单情况：两者都是单一标记类型
            if not isinstance(gen_mark, list) and not isinstance(ref_mark, list):
                return 1.0 if gen_mark == ref_mark else 0.0
            
            # 复杂情况：分层图表
            if isinstance(gen_mark, list) and isinstance(ref_mark, list):
                # 如果图层数量不同，返回0
                if len(gen_mark) != len(ref_mark):
                    return 0.0
                    
                # 所有图层必须在类型上匹配（忽略样式属性）
                for gen_mark_single, ref_mark_single in zip(gen_mark, ref_mark):
                    # 如果是对象，提取类型
                    gen_type = gen_mark_single.get('type') if isinstance(gen_mark_single, dict) else gen_mark_single
                    ref_type = ref_mark_single.get('type') if isinstance(ref_mark_single, dict) else ref_mark_single
                    
                    # 如果任一图层类型不匹配，返回0
                    if gen_type != ref_type:
                        return 0.0
                
                # 所有图层类型匹配，返回1.0
                return 1.0
            
            # 一个是单一标记，一个是图层标记，不匹配
            return 0.0
        except Exception as e:
            _log(f"Error in evaluate_chart_type_match: {str(e)}")
            return 0.0

    @staticmethod
    def evaluate_data_mapping(generated_chart: alt.Chart, reference_chart: alt.Chart) -> float:
        """
        评估生成图表与参考图表之间的数据映射准确性。
        返回0到1之间的分数表示数据映射准确度。
        
        参数:
            generated_chart: AI生成的Altair图表
            reference_chart: 参考Altair图表
            
        返回:
            数据映射准确度得分，1.0表示完全匹配，0.0表示完全不匹配
        """
        try:
            # 获取图表中的数据
            gen_data_df = generated_chart.data
            ref_data_df = reference_chart.data

            # 处理图表对象中数据不直接可用的情况
            if type(ref_data_df) is alt.utils.schemapi.UndefinedType or type(gen_data_df) is alt.utils.schemapi.UndefinedType:
                try:
                    # 尝试从图表字典中获取数据
                    gen_chart_dict = generated_chart.to_dict()
                    ref_chart_dict = reference_chart.to_dict()
                    
                    # 提取可用的数据集
                    gen_data_json = list(gen_chart_dict.get('datasets', {}).values())
                    ref_data_json = list(ref_chart_dict.get('datasets', {}).values())
                    
                    # 比较数据集
                    if gen_data_json and ref_data_json and gen_data_json == ref_data_json:
                        return 1.0
                    
                    # 如果数据集比较失败，尝试从图表字典中提取数据
                    if 'data' in gen_chart_dict and 'data' in ref_chart_dict:
                        gen_data_values = gen_chart_dict['data'].get('values', [])
                        ref_data_values = ref_chart_dict['data'].get('values', [])
                        if gen_data_values and ref_data_values and gen_data_values == ref_data_values:
                            return 1.0
                    
                    # 如果仍然未匹配，尝试在图层对象中查找数据
                    if 'layer' in gen_chart_dict and 'layer' in ref_chart_dict:
                        # 提取具有数据的第一个图层
                        for gen_layer in gen_chart_dict['layer']:
                            if 'data' in gen_layer and 'values' in gen_layer['data']:
                                gen_layer_data = gen_layer['data']['values']
                                for ref_layer in ref_chart_dict['layer']:
                                    if 'data' in ref_layer and 'values' in ref_layer['data']:
                                        ref_layer_data = ref_layer['data']['values']
                                        if gen_layer_data == ref_layer_data:
                                            return 1.0
                    
                    # 如果仍然没有匹配，返回0
                    return 0.0
                    
                except Exception as e:
                    _log(f"Error comparing datasets: {str(e)}")
                    return 0.0
                    
            elif hasattr(gen_data_df, 'equals') and hasattr(ref_data_df, 'equals'):
                # 第一尝试：直接DataFrame比较
                if gen_data_df.equals(ref_data_df):
                    return 1.0
                else:
                    # 尝试更健壮的比较方法
                    try:
                        # 用于标准化和规范化DataFrame进行比较的函数
                        def standardize_df(df):
                            """
                            标准化DataFrame以便进行比较。
                            重置索引并将所有列转换为字符串类型。
                            
                            参数:
                                df: 要标准化的DataFrame
                                
                            返回:
                                标准化后的DataFrame
                            """
                            # 重置索引以避免索引比较问题
                            df_reset = df.reset_index(drop=True)
                            # 将所有列转换为字符串以避免类型比较问题
                            for col in df_reset.columns:
                                df_reset[col] = df_reset[col].astype(str)
                            return df_reset
                        
                        # 标准化DataFrame
                        gen_std = standardize_df(gen_data_df)
                        ref_std = standardize_df(ref_data_df)
                        
                        # 首先检查形状（行数和列数应该相同）
                        if gen_std.shape == ref_std.shape:
                            # 情况1：列名和顺序完全相同的比较
                            if list(gen_std.columns) == list(ref_std.columns) and gen_std.equals(ref_std):
                                return 1.0
                            
                            # 情况2：列名相同但顺序不同的比较
                            if set(gen_std.columns) == set(ref_std.columns):
                                # 重新排序参考列以匹配生成列
                                ref_reordered = ref_std[gen_std.columns]
                                if gen_std.equals(ref_reordered):
                                    return 1.0
                            
                            # 情况3：不考虑列名比较数据内容
                            # 转换为字符串表示并排序行进行内容比较
                            gen_rows = sorted([','.join(row) for row in gen_std.values.astype(str)])
                            ref_rows = sorted([','.join(row) for row in ref_std.values.astype(str)])
                            
                            if gen_rows == ref_rows:
                                return 1.0
                            
                            # 情况4：通过排序值比较各个列
                            # 当列重新排序或重命名时，这很有用
                            gen_sorted_cols = [sorted(gen_std.iloc[:, i].astype(str).tolist()) for i in range(gen_std.shape[1])]
                            ref_sorted_cols = [sorted(ref_std.iloc[:, i].astype(str).tolist()) for i in range(ref_std.shape[1])]
                            
                            # 将列数据转换为可哈希的元组
                            gen_cols_set = {tuple(col) for col in gen_sorted_cols}
                            ref_cols_set = {tuple(col) for col in ref_sorted_cols}
                            
                            if gen_cols_set == ref_cols_set:
                                return 1.0
                        
                        # 分层图表的特殊情况
                        if hasattr(generated_chart, 'layer') and hasattr(reference_chart, 'layer'):
                            # 从每个图层提取数据
                            gen_layers_data = [layer.data for layer in generated_chart.layer if hasattr(layer, 'data')]
                            ref_layers_data = [layer.data for layer in reference_chart.layer if hasattr(layer, 'data')]
                            
                            # 比较每个图层组合的数据
                            for gen_layer_data in gen_layers_data:
                                if isinstance(gen_layer_data, pd.DataFrame):
                                    gen_layer_std = standardize_df(gen_layer_data)
                                    for ref_layer_data in ref_layers_data:
                                        if isinstance(ref_layer_data, pd.DataFrame):
                                            ref_layer_std = standardize_df(ref_layer_data)
                                            try:
                                                # 首先尝试直接比较
                                                if gen_layer_std.equals(ref_layer_std):
                                                    return 1.0
                                                
                                                # 然后尝试内容比较
                                                gen_layer_rows = sorted([','.join(row) for row in gen_layer_std.values.astype(str)])
                                                ref_layer_rows = sorted([','.join(row) for row in ref_layer_std.values.astype(str)])
                                                if gen_layer_rows == ref_layer_rows:
                                                    return 1.0
                                            except:
                                                continue
                        
                        # 如果所有比较都失败，记录失败原因并返回0
                        _log(f"Column comparison failed: {gen_std.shape} vs {ref_std.shape}")
                        _log(f"Gen columns: {gen_std.columns.tolist()}")
                        _log(f"Ref columns: {ref_std.columns.tolist()}")
                        return 0.0
                    
                    except Exception as e:
                        _log(f"Error in data comparison: {str(e)}\n{traceback.format_exc()}")
                        return 0.0
            else:
                # 处理既不能直接属性比较也不能DataFrame比较的情况
                try:
                    # 转换为JSON/dict表示并比较
                    gen_dict = generated_chart.to_dict()
                    ref_dict = reference_chart.to_dict()
                    
                    # 尝试直接比较数据值
                    if 'data' in gen_dict and 'data' in ref_dict:
                        gen_values = gen_dict['data'].get('values', [])
                        ref_values = ref_dict['data'].get('values', [])
                        
                        if gen_values == ref_values:
                            return 1.0
                    
                    # 回退到直接相等性检查
                    return 1.0 if gen_data_df == ref_data_df else 0.0
                except Exception:
                    return 0.0
        except Exception as e:
            _log(f"Error in evaluate_data_mapping: {str(e)}\n{traceback.format_exc()}")
            return 0.0

    @staticmethod
    def evaluate_encoding_consistency(generated_chart: alt.Chart, reference_chart: alt.Chart) -> float:
        """
        评估生成图表与参考图表之间的视觉编码一致性。
        返回0到1之间的分数表示编码一致性。
        
        参数:
            generated_chart: AI生成的Altair图表
            reference_chart: 参考Altair图表
            
        返回:
            编码一致性得分，1.0表示完全一致，0.0表示完全不一致
        """
        try:
            def extract_encoding(chart: alt.Chart) -> Union[dict, List[dict]]:
                """
                从图表中提取编码信息。
                
                参数:
                    chart: Altair图表对象
                    
                返回:
                    编码字典或图层编码列表
                """
                try:
                    if hasattr(chart, 'encoding') and type(chart.encoding) is not alt.utils.schemapi.UndefinedType:
                        return chart.encoding
                    elif hasattr(chart, 'layer') and chart.layer:
                        return [extract_encoding(layer) for layer in chart.layer]
                    return None
                except Exception as e:
                    _log(f"Error extracting encoding: {str(e)}")
                    return None
            
            # 从两个图表中提取编码
            gen_encoding = extract_encoding(generated_chart)
            ref_encoding = extract_encoding(reference_chart)

            # 检查编码是否存在
            if gen_encoding is None or ref_encoding is None:
                _log("One or both encodings are None")
                return 0.0
                
            _log(f"Original gen_encoding: {gen_encoding}")
            _log(f"Original ref_encoding: {ref_encoding}")

            # 将编码规范化为标准形式以进行比较
            def normalize_encoding(encoding):
                """
                将编码转换为带有详细配置的标准化字典表示。
                
                参数:
                    encoding: 要规范化的编码对象
                    
                返回:
                    规范化后的编码字典
                """
                # 如果是列表，递归处理每个元素
                if isinstance(encoding, list):
                    return [normalize_encoding(e) for e in encoding]
                
                # 对于Altair编码对象，转换为字典
                try:
                    if hasattr(encoding, 'to_dict'):
                        encoding_dict = encoding.to_dict()
                    else:
                        encoding_dict = dict(encoding)
                except:
                    _log(f"Failed to convert encoding to dict: {encoding}")
                    return {}
                
                # 提取详细的通道配置
                normalized = {}
                for channel, config in encoding_dict.items():
                    if isinstance(config, dict):
                        # 创建详细的通道表示
                        channel_info = {}
                        
                        # 提取类型（quantitative, nominal等）
                        if 'type' in config:
                            channel_info['type'] = config['type']
                        elif 'shorthand' in config and ':' in config['shorthand']:
                            # 从简写表示法提取类型（field:type）
                            channel_info['type'] = config['shorthand'].split(':')[-1]
                        
                        # 如果可用，提取字段名称
                        if 'field' in config:
                            channel_info['field'] = config['field']
                        elif 'shorthand' in config and ':' in config['shorthand']:
                            channel_info['field'] = config['shorthand'].split(':')[0]
                        
                        # 保留聚合函数（sum, count等）
                        if 'aggregate' in config:
                            channel_info['aggregate'] = config['aggregate']
                            
                        # 保留分箱设置（true/false）
                        if 'bin' in config:
                            channel_info['bin'] = config['bin']
                            
                        # 如果存在，保留堆叠设置
                        if 'stack' in config:
                            channel_info['stack'] = config['stack']
                            
                        # 保留比例配置
                        if 'scale' in config:
                            channel_info['scale'] = config['scale']
                            
                        # 保留坐标轴/图例配置
                        if 'axis' in config:
                            channel_info['axis'] = config['axis']
                            
                        if 'legend' in config:
                            channel_info['legend'] = config['legend']
                            
                        # 保留标题
                        if 'title' in config:
                            channel_info['title'] = config['title']
                            
                        # 添加到规范化编码
                        normalized[channel] = channel_info
                    else:
                        # 简单值，保持原样
                        normalized[channel] = {"value": config}
                
                return normalized
            
            # 规范化两个编码
            norm_gen = normalize_encoding(gen_encoding)
            norm_ref = normalize_encoding(ref_encoding)
            
            # 定义一个辅助函数，基于核心属性比较通道配置
            def is_channel_equal(enc1, enc2):
                """
                比较两个通道配置的核心属性是否相等。
                
                参数:
                    enc1: 第一个通道配置
                    enc2: 第二个通道配置
                    
                返回:
                    布尔值，表示核心属性是否相等
                """
                # 仅比较数据处理评估的类型
                return enc1.get('type') == enc2.get('type')
            
            # 定义一个函数，在每一层比较编码
            def compare_encodings(gen_enc, ref_enc):
                """
                比较两个编码的结构一致性。
                
                参数:
                    gen_enc: 生成图表的编码
                    ref_enc: 参考图表的编码
                    
                返回:
                    1.0表示完全匹配，0.0表示不匹配
                """
                # 处理不同类型的编码（列表vs字典）
                if isinstance(gen_enc, list) and isinstance(ref_enc, list):
                    # 如果两者都是分层图表，层必须匹配
                    if len(gen_enc) != len(ref_enc):
                        _log(f"Layer count mismatch: {len(gen_enc)} vs {len(ref_enc)}")
                        return 0.0  # 层不匹配，返回0
                    
                    # 所有层必须完全匹配才能得到1分
                    for gen_layer, ref_layer in zip(gen_enc, ref_enc):
                        if compare_encodings(gen_layer, ref_layer) == 0.0:
                            return 0.0  # 任何层不匹配意味着整体不匹配
                    
                    return 1.0  # 所有层匹配
                
                elif isinstance(gen_enc, dict) and isinstance(ref_enc, dict):
                    # 如果两者都是单一图表，比较通道
                    all_channels = set(gen_enc.keys()).union(set(ref_enc.keys()))
                    if not all_channels:
                        return 1.0  # 两者都没有编码
                    
                    # 检查每个通道是否完全匹配
                    for channel in all_channels:
                        # 如果通道在两者中都存在且类型匹配
                        if not (channel in gen_enc and channel in ref_enc and is_channel_equal(gen_enc[channel], ref_enc[channel])):
                            return 0.0  # 任何不匹配意味着整体不匹配
                    
                    return 1.0  # 所有通道匹配
                
                elif isinstance(gen_enc, list) and isinstance(ref_enc, dict):
                    # 生成是分层的，参考是单一的 - 视为不匹配
                    return 0.0
                
                elif isinstance(gen_enc, dict) and isinstance(ref_enc, list):
                    # 生成是单一的，参考是分层的 - 视为不匹配
                    return 0.0
                
                else:
                    # 意外的编码类型
                    _log(f"Unexpected encoding types: {type(gen_enc)} vs {type(ref_enc)}")
                    return 0.0
            
            # 比较编码并获取最终得分
            final_score = compare_encodings(norm_gen, norm_ref)
            _log(f"Final encoding score: {final_score}")
            
            return final_score
            
        except Exception as e:
            _log(f"Error in evaluate_encoding_consistency: {str(e)}\n{traceback.format_exc()}")
            return 0.0

    @staticmethod
    def evaluate_interaction_implementation(generated_chart: alt.Chart, reference_chart: alt.Chart) -> float:
        """
        评估生成图表与参考图表之间的交互功能实现一致性。
        
        参数:
            generated_chart: AI生成的Altair图表
            reference_chart: 参考Altair图表
            
        返回:
            交互实现准确度得分，1.0表示完全匹配，0.0表示不匹配
        """
        try:
            # 获取两个图表的交互参数
            gen_params = getattr(generated_chart, 'params', None)
            ref_params = getattr(reference_chart, 'params', None)
            
            # 检查是否两个图表都没有交互功能
            if gen_params is None and ref_params is None:
                return 1.0  # 两个图表都是非交互式的，视为匹配
            elif gen_params is None or ref_params is None:
                return 0.0  # 一个图表有交互功能而另一个没有，视为不匹配
            
            # 比较交互参数是否完全相同    
            return 1.0 if gen_params == ref_params else 0.0
        except Exception as e:
            _log(f"Error in evaluate_interaction_implementation: {str(e)}")
            return 0.0

    @staticmethod
    def evaluate_chart_config(generated_chart: alt.Chart, reference_chart: alt.Chart) -> float:
        """
        评估生成图表与参考图表之间的图表配置相似性。
        
        参数:
            generated_chart: AI生成的Altair图表
            reference_chart: 参考Altair图表
            
        返回:
            配置相似性得分，1.0表示完全相同，0.0表示不同
        """
        try:
            # 获取两个图表的配置
            gen_config = getattr(generated_chart, 'config', None)
            ref_config = getattr(reference_chart, 'config', None)
            
            # 检查如果两个图表都没有自定义配置
            if gen_config is None and ref_config is None:
                return 1.0  # 两个图表都使用默认配置，视为匹配
            elif gen_config is None or ref_config is None:
                return 0.0  # 一个图表有自定义配置而另一个没有，视为不匹配
            
            # 比较配置是否完全相同    
            return 1.0 if gen_config == ref_config else 0.0
        except Exception as e:
            _log(f"Error in evaluate_chart_config: {str(e)}")
            return 0.0

    @staticmethod
    def evaluate_transform(generated_chart: alt.Chart, reference_chart: alt.Chart) -> float:
        """
        评估生成图表与参考图表之间的转换配置准确性。
        转换配置包括数据过滤、聚合、计算等操作。
        完全匹配返回1.0，否则返回0.0。
        
        参数:
            generated_chart: AI生成的Altair图表
            reference_chart: 参考Altair图表
            
        返回:
            转换配置准确性得分，1.0表示完全匹配，0.0表示不匹配
        """
        try:
            def extract_transform(chart: alt.Chart) -> Union[str, List[str]]:
                """
                从图表中提取转换配置。
                
                参数:
                    chart: Altair图表对象
                    
                返回:
                    转换配置或图层转换配置列表
                """
                try:
                    if hasattr(chart, 'transform') and type(chart.transform) is not alt.utils.schemapi.UndefinedType:
                        return chart.transform
                    elif hasattr(chart, 'layer') and chart.layer:
                        transforms = []
                        for layer in chart.layer:
                            layer_transform = extract_transform(layer)
                            if layer_transform is not None:
                                transforms.append(layer_transform)
                        # 只有当transforms不为空时才返回，否则返回None
                        return transforms if transforms else None
                    return None
                except Exception as e:
                    _log(f"Error extracting transform: {str(e)}")
                    return None
            
            # 提取两个图表的转换配置
            gen_transform = extract_transform(generated_chart)
            ref_transform = extract_transform(reference_chart)
            
            # 添加调试日志
            _log(f"Generated transform: {gen_transform}")
            _log(f"Reference transform: {ref_transform}")
            
            # 检查转换配置是否存在
            if gen_transform is None or ref_transform is None:
                # 如果两者都没有转换配置，视为匹配（两个图表都不使用转换）
                if gen_transform is None and ref_transform is None:
                    return 1.0
                # 一个有转换配置而另一个没有，视为不匹配
                return 0.0
            
            # 空列表视为没有转换操作，与None等效处理
            if isinstance(gen_transform, list) and not gen_transform and isinstance(ref_transform, list) and not ref_transform:
                return 1.0
            
            # 处理单一转换配置的情况
            if not isinstance(gen_transform, list) and not isinstance(ref_transform, list):
                # 清理并比较转换配置
                sanitized_gen = LowLevelMetrics._sanitize_dict(gen_transform)
                sanitized_ref = LowLevelMetrics._sanitize_dict(ref_transform)
                return 1.0 if sanitized_gen == sanitized_ref else 0.0
            
            # 处理分层图表的转换配置
            if isinstance(gen_transform, list) and isinstance(ref_transform, list):
                # 如果转换配置数量不同，视为不匹配
                if len(gen_transform) != len(ref_transform):
                    return 0.0
                
                # 检查每个转换元素是否匹配
                for gen_item, ref_item in zip(gen_transform, ref_transform):
                    sanitized_gen_item = LowLevelMetrics._sanitize_dict(gen_item)
                    sanitized_ref_item = LowLevelMetrics._sanitize_dict(ref_item)
                    if sanitized_gen_item != sanitized_ref_item:
                        return 0.0
                
                # 所有元素都匹配，返回1.0
                return 1.0
            
            # 一个是单一转换配置，一个是列表，视为不匹配
            return 0.0
        except Exception as e:
            _log(f"Error in evaluate_transform: {str(e)}\n{traceback.format_exc()}")
            return 0.0

    @staticmethod
    def evaluate_all_metrics(generated_chart: alt.Chart, reference_chart: alt.Chart) -> Tuple[Dict[str, float], float]:
        """
        评估所有低级指标并返回它们的得分。
        
        参数:
            generated_chart: AI生成的Altair图表
            reference_chart: 参考Altair图表
            
        返回:
            包含每个指标得分的字典和总体得分
        """
        try:
            # 计算各个指标，每个指标都有异常处理
            try:
                # 图表类型匹配评估
                chart_type_score = LowLevelMetrics.evaluate_chart_type_match(generated_chart, reference_chart)
            except Exception as e:
                _log(f"Error evaluating chart type: {str(e)}")
                chart_type_score = 0.0
                
            try:
                # 数据映射评估
                data_mapping_score = LowLevelMetrics.evaluate_data_mapping(generated_chart, reference_chart)
            except Exception as e:
                _log(f"Error evaluating data mapping: {str(e)}")
                data_mapping_score = 0.0
                
            try:
                # 编码一致性评估
                encoding_score = LowLevelMetrics.evaluate_encoding_consistency(generated_chart, reference_chart)
            except Exception as e:
                _log(f"Error evaluating encoding: {str(e)}")
                encoding_score = 0.0
                
            try:
                # 交互实现评估
                interaction_score = LowLevelMetrics.evaluate_interaction_implementation(generated_chart, reference_chart)
            except Exception as e:
                _log(f"Error evaluating interaction: {str(e)}")
                interaction_score = 0.0
                
            try:
                # 图表配置评估
                config_score = LowLevelMetrics.evaluate_chart_config(generated_chart, reference_chart)
            except Exception as e:
                _log(f"Error evaluating config: {str(e)}")
                config_score = 0.0
                
            try:
                # 转换评估
                transform_score = LowLevelMetrics.evaluate_transform(generated_chart, reference_chart)
            except Exception as e:
                _log(f"Error evaluating transform: {str(e)}")
                transform_score = 0.0
            
            # 计算总体得分 - 确保至少有一个有效指标
            valid_metrics = 0
            total_score = 0.0
            
            # 汇总所有指标得分
            metrics = {
                'chart_type': chart_type_score,
                'data_mapping': data_mapping_score,
                'encoding': encoding_score,
                'interaction': interaction_score,
                'config': config_score,
                'transform': transform_score
            }
            
            # 计算有效指标的平均分数
            for score in metrics.values():
                if score is not None:
                    valid_metrics += 1
                    total_score += score
            
            # 如果没有有效指标，总体得分为0
            if valid_metrics == 0:
                overall_score = 0.0
            else:
                # 总体得分为所有有效指标的平均值
                overall_score = total_score / valid_metrics
                
            # 将总体得分添加到指标字典中
            metrics['overall'] = overall_score
            
            return metrics, overall_score
        except Exception as e:
            _log(f"Fatal error in evaluate_all_metrics: {str(e)}")
            # 发生致命错误时返回空得分和0总分
            return {
                'chart_type': 0.0,
                'data_mapping': 0.0,
                'encoding': 0.0,
                'interaction': 0.0,
                'config': 0.0,
                'transform': 0.0,
                'overall': 0.0
            }, 0.0

# ======================================== High-Level Metrics ========================================

class HighLevelMetrics:
    """
    High-Level Metrics for evaluating visualizations based on their rendered images using LLMs.
    """

    @staticmethod
    def evaluate_charts(reference_chart: alt.Chart, generated_chart: alt.Chart) -> Tuple[Dict[str, Union[float, Dict[str, float]]], float]:
        """
        Evaluates the similarity between reference and generated visualization images using an LLM.
        
        Args:
            reference_chart: The reference Altair chart
            generated_chart: The AI-generated Altair chart
            
        Returns:
            A dictionary containing the evaluation results
        """
        default_scores = {
            'chart_types': 0.0,
            'layout': 0.0,
            'text_content': 0.0,
            'data': 0.0,
            'style': 0.0,
            'clarity': 0.0,
            'overall': 0.0
        }
        
        try:
            _log("Converting charts to images...")
            
            # 转换参考图表，使用更小的尺寸限制（2MB）
            status_ref, reference_chart_img_url = chart_to_img_url(reference_chart, max_size_mb=2.0)
            if not status_ref:
                _log("ERROR: Failed to convert reference chart to image")
                return default_scores, 0.0
            
            # 转换生成的图表
            status_gen, generated_chart_img_url = chart_to_img_url(generated_chart, max_size_mb=2.0)
            if not status_gen:
                _log("ERROR: Failed to convert generated chart to image")
                return default_scores, 0.0

            prompt = HIGH_LEVEL_PROMPT
            
            messages = [
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "text",
                            "text": prompt
                        },
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": reference_chart_img_url
                            }
                        },
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": generated_chart_img_url
                            }
                        }
                    ]
                }
            ]

            _log(f"High-Level Metrics - Calling LLM...")
            
            try:
                response = call_llm(messages)
                _log(f"High-Level Metrics Response received (length: {len(response)})")
                _log(f"High-Level Metrics Response: \n{response}")
            except Exception as e:
                error_msg = f"ERROR: Failed to get LLM response: {str(e)}"
                _log(error_msg)
                print(error_msg)
                return default_scores, 0.0

            # Parse the response to extract scores
            scores = HighLevelMetrics.parse_response(response)
            _log(f"Parsed scores: {scores}")
            return scores, scores['overall']
            
        except Exception as e:
            error_msg = f"ERROR in evaluate_charts: {str(e)}\n{traceback.format_exc()}"
            _log(error_msg)
            print(error_msg)
            return default_scores, 0.0

    @staticmethod
    def parse_response(response: str) -> Dict[str, float]:
        """
        Parse the JSON response from the LLM evaluation.
        
        Args:
            response: The LLM's JSON response string
            
        Returns:
            A dictionary containing normalized scores (0-1 range) for each category
        """
        scores = {
            'chart_types': 0.0,
            'layout': 0.0,
            'text_content': 0.0,
            'data': 0.0,
            'style': 0.0,
            'clarity': 0.0,
            'overall': 0.0
        }
        
        try:
            # Try to find and extract JSON from the response
            response = response.strip()
            
            # Find JSON content if there's text around it
            json_start = response.find('{')
            json_end = response.rfind('}') + 1
            
            if json_start >= 0 and json_end > json_start:
                json_str = response[json_start:json_end]
                result = json.loads(json_str)
                
                # Extract and normalize scores to 0-1 range
                for category in scores.keys():
                    if category in result:
                        cat_score = result[category].get('score', 0)
                        cat_max = result[category].get('max', 1)
                        scores[category] = float(cat_score) / float(cat_max)
            else:
                _log("WARNING: No valid JSON found in the response")
                # Fall back to the old parsing method if JSON not found
                lines = response.strip().split('\n')
                for line in lines:
                    if 'Chart Types:' in line and '/' in line:
                        scores['chart_types'] = float(line.split('/')[-1].strip()) / 20.0
                    elif 'Layout:' in line and '/' in line:
                        scores['layout'] = float(line.split('/')[-1].strip()) / 10.0
                    elif 'Text Content:' in line and '/' in line:
                        scores['text_content'] = float(line.split('/')[-1].strip()) / 20.0
                    elif 'Data:' in line and '/' in line:
                        scores['data'] = float(line.split('/')[-1].strip()) / 20.0
                    elif 'Style:' in line and '/' in line:
                        scores['style'] = float(line.split('/')[-1].strip()) / 20.0
                    elif 'Clarity:' in line and '/' in line:
                        scores['clarity'] = float(line.split('/')[-1].strip()) / 10.0
                    elif 'Score:' in line and '/' in line:
                        # Extract only the numeric part before converting to float
                        score_text = line.split('/')[-1].strip()
                        # Remove any non-numeric characters
                        score_text = ''.join(c for c in score_text if c.isdigit() or c == '.')
                        scores['overall'] = float(score_text) / 100.0
        
        except Exception as e:
            _log(f"ERROR: Error parsing evaluation response: {str(e)}\nResponse: {response}")
        
        return scores

# ======================================== Visualization Metrics ========================================

def process_single_result(result, use_high_level_metrics=True, use_low_level_metrics=True):
    """改进版的单个结果处理函数"""
    try:
        prediction = result['prediction']
        reference = result['label']

        if prediction == "":
            return {
                'result_id': result.get('id', None),
                'log': 'Prediction is empty',
                'status': 'wrong',
                'low_level_scores': None,
                'high_level_scores': None,
                'low_level_overall_score': 0.0,
                'high_level_overall_score': 0.0
            }
        
        # 执行预测代码，并捕获所有可能的异常
        try:
            pred_chart_obj = exec_altair_code(prediction)
            if not pred_chart_obj['success'] or pred_chart_obj['chart'] is None:
                error_msg = pred_chart_obj.get('error', 'Unknown error')
                _log(f"Generated chart execution failed: {error_msg}")
                return {
                    'result_id': result.get('id', None),
                    'log': f"Generated chart is not valid: {error_msg}",
                    'status': 'wrong',
                    'low_level_scores': None,
                    'high_level_scores': None,
                    'low_level_overall_score': 0.0,
                    'high_level_overall_score': 0.0
                }
            pred_chart = pred_chart_obj['chart']
        except Exception as e:
            _log(f"Exception during prediction execution: {str(e)}")
            return {
                'result_id': result.get('id', None),
                'log': f"Exception during prediction execution: {str(e)}",
                'status': 'wrong',
                'low_level_scores': None,
                'high_level_scores': None,
                'low_level_overall_score': 0.0,
                'high_level_overall_score': 0.0
            }
            
        # Process reference chart
        ref_chart_obj = exec_altair_code(reference)
        # if not ref_chart_obj['success'] or ref_chart_obj['chart'] is None:
        #     _log(f"Reference chart execution failed: {ref_chart_obj.get('error', 'Unknown error')}")
        #     return {
        #         'result_id': result.get('id', None),
        #         'log': f"Reference chart is not valid: {ref_chart_obj.get('error', 'Unknown error')}",
        #         'status': 'wrong',
        #         'low_level_scores': None,
        #         'high_level_scores': None,
        #         'low_level_overall_score': 0.0,
        #         'high_level_overall_score': 0.0
        #     }
        ref_chart = ref_chart_obj['chart']

        # Log chart info for debugging
        try:
            _log(f"Generated chart: {type(pred_chart)}")
            _log(f"Reference chart: {type(ref_chart)}")
            
            # Log encodings to help understand what's being compared
            if hasattr(pred_chart, 'encoding'):
                _log(f"Generated encoding: {pred_chart.encoding}")
            if hasattr(ref_chart, 'encoding'):
                _log(f"Reference encoding: {ref_chart.encoding}")
        except Exception as e:
            _log(f"Error logging chart info: {str(e)}")

        # Create instances of metrics classes
        low_level_metrics = LowLevelMetrics()
        high_level_metrics = HighLevelMetrics()

        # Evaluate low-level metrics
        low_level_scores, low_level_overall_score = {}, 0.0
        if use_low_level_metrics:
            try:
                low_level_scores, low_level_overall_score = low_level_metrics.evaluate_all_metrics(pred_chart, ref_chart)
                _log(f"Low-level metrics: {low_level_scores}")
                _log(f"Low-level overall score: {low_level_overall_score}")
            except Exception as e:
                _log(f"Error in low-level metrics evaluation: {str(e)}\n{traceback.format_exc()}")
                low_level_scores = {
                    'chart_type': 0.0,
                    'data_mapping': 0.0,
                    'encoding': 0.0,
                    'interaction': 0.0,
                    'config': 0.0,
                    'transform': 0.0,
                    'overall': 0.0
                }
                low_level_overall_score = 0.0

        # Evaluate high-level metrics
        high_level_scores, high_level_overall_score = {}, 0.0
        if use_high_level_metrics:
            try:
                high_level_scores, high_level_overall_score = high_level_metrics.evaluate_charts(ref_chart, pred_chart)
                _log(f"High-level metrics: {high_level_scores}")
                _log(f"High-level overall score: {high_level_overall_score}")
            except Exception as e:
                _log(f"Error in high-level metrics evaluation: {str(e)}\n{traceback.format_exc()}")
                high_level_scores = {
                    'chart_types': 0.0,
                    'layout': 0.0,
                    'text_content': 0.0,
                    'data': 0.0,
                    'style': 0.0,
                    'clarity': 0.0,
                    'overall': 0.0
                }
                high_level_overall_score = 0.0

        # Determine result status based on selected metrics
        status = 'wrong'
        log_message = ''
        
        if use_high_level_metrics and not use_low_level_metrics:
            if high_level_overall_score <= 0.8:
                log_message = 'High-level metrics score is lower than 0.8'
                status = 'wrong'
            else:
                log_message = 'High-level metrics score is higher than 0.8'
                status = 'correct'
        elif use_low_level_metrics and not use_high_level_metrics:
            if low_level_overall_score <= 0.8:
                log_message = f'Low-level metrics score is lower than 0.8: {low_level_scores}'
                status = 'wrong'
            else:
                log_message = f'Low-level metrics score is higher than 0.8: {low_level_scores}'
                status = 'correct'
        else:
            # Both metrics are used
            combined_score = (low_level_overall_score + high_level_overall_score) / 2.0
            if combined_score <= 0.8:
                log_message = f'Combined metrics score is lower than 0.8. Low-level: {low_level_scores}, High-level: {high_level_scores}'
                status = 'wrong'
            else:
                log_message = f'Combined metrics score is higher than 0.8. Low-level: {low_level_scores}, High-level: {high_level_scores}'
                status = 'correct'

        # Don't return the original result or chart objects to avoid pickling issues
        return {
            'result_id': result.get('id', None),
            'log': log_message,
            'status': status,
            'low_level_scores': low_level_scores,
            'high_level_scores': high_level_scores,
            'low_level_overall_score': low_level_overall_score,
            'high_level_overall_score': high_level_overall_score
        }
    except Exception as e:
        # 捕获整个函数中的任何异常
        _log(f"Fatal error in process_single_result: {str(e)}\n{traceback.format_exc()}")
        return {
            'result_id': result.get('id', None),
            'log': f"Fatal error: {str(e)}",
            'status': 'wrong',
            'low_level_scores': None,
            'high_level_scores': None,
            'low_level_overall_score': 0.0,
            'high_level_overall_score': 0.0
        }

class VisualizationMetrics:
    """
    Combined metrics for evaluating text-to-visualization systems.
    """
    def __init__(self):
        self.low_level_metrics = LowLevelMetrics()
        self.high_level_metrics = HighLevelMetrics()
        self.metrics = {
            "low_level_scores": {
                "chart_type": 0.0,
                "data_mapping": 0.0,
                "encoding": 0.0,
                "interaction": 0.0,
                "config": 0.0,
                "transform": 0.0,
                "overall": 0.0
            },
            "high_level_scores": {
                "chart_types": 0.0,
                "layout": 0.0,
                "text_content": 0.0,
                "data": 0.0,
                "style": 0.0,
                "clarity": 0.0,
                "overall": 0.0
            },
            "low_level_overall_score": 0.0,
            "high_level_overall_score": 0.0,
            "combined_overall_score": 0.0
        }

    def evaluate(
        self,
        results: List[Dict[str, Any]],
        use_high_level_metrics: bool = True,
        use_low_level_metrics: bool = True
    ) -> Dict[str, Any]:
        """
        Evaluates both low-level and high-level metrics if available.
        
        Args:
            results: The list of results containing predictions and references
            use_high_level_metrics: Whether to use high-level metrics
            use_low_level_metrics: Whether to use low-level metrics
            
        Returns:
            A dictionary containing all evaluation results
        """
        wrong_results = []
        correct_results = []
        for result in tqdm(results, total=len(results)):
            prediction = result['prediction']
            reference = result['label']
            # Process generated chart
            pred_chart_obj = exec_altair_code(prediction)
            if not pred_chart_obj['success'] or pred_chart_obj['chart'] is None:
                result['log'] = 'Generated chart is not valid'
                wrong_results.append(result)
                continue
            pred_chart = pred_chart_obj['chart']
            
            # Process reference chart in a separate process
            ref_chart_obj = exec_altair_code(reference)
            if not ref_chart_obj['success'] or ref_chart_obj['chart'] is None:
                result['log'] = 'Reference chart is not valid'
                wrong_results.append(result)
                continue
            ref_chart = ref_chart_obj['chart']

            # Evaluate low-level metrics
            low_level_scores, low_level_overall_score = {}, 0.0
            if use_low_level_metrics:
                low_level_scores, low_level_overall_score = self.low_level_metrics.evaluate_all_metrics(pred_chart, ref_chart)

            # Evaluate high-level metrics
            high_level_scores, high_level_overall_score = {}, 0.0
            if use_high_level_metrics:
                high_level_scores, high_level_overall_score = self.high_level_metrics.evaluate_charts(ref_chart, pred_chart)

            # Determine result status based on selected metrics
            if use_high_level_metrics and not use_low_level_metrics:
                if high_level_overall_score <= 0.8:
                    result['log'] = 'High-level metrics score is lower than 0.8'
                    wrong_results.append(result)
                else:
                    result['log'] = 'High-level metrics score is higher than 0.8'
                    correct_results.append(result)
            elif use_low_level_metrics and not use_high_level_metrics:
                if low_level_overall_score <= 0.8:
                    result['log'] = 'Low-level metrics score is lower than 0.8'
                    wrong_results.append(result)
                else:
                    result['log'] = 'Low-level metrics score is higher than 0.8'
                    correct_results.append(result)
            else:
                # Both metrics are used
                combined_score = (low_level_overall_score + high_level_overall_score) / 2.0
                if combined_score <= 0.8:
                    result['log'] = 'Combined metrics score is lower than 0.8'
                    wrong_results.append(result)
                else:
                    result['log'] = 'Combined metrics score is higher than 0.8'
                    correct_results.append(result)

            if use_low_level_metrics:
                self.metrics['low_level_overall_score'] += low_level_overall_score / len(results)
                for key, value in low_level_scores.items():
                    self.metrics['low_level_scores'][key] += value / len(results)
                    
            if use_high_level_metrics:
                self.metrics['high_level_overall_score'] += high_level_overall_score / len(results)
                for key, value in high_level_scores.items():
                    self.metrics['high_level_scores'][key] += value / len(results)
            
            if use_low_level_metrics and use_high_level_metrics:
                self.metrics['combined_overall_score'] += (low_level_overall_score + high_level_overall_score) / 2.0 / len(results)
            elif use_low_level_metrics:
                self.metrics['combined_overall_score'] += low_level_overall_score / len(results)
            elif use_high_level_metrics:
                self.metrics['combined_overall_score'] += high_level_overall_score / len(results)
                
        return self.metrics, wrong_results, correct_results
    
    def evaluate_parallel(
        self,
        results: List[Dict[str, Any]],
        use_high_level_metrics: bool = True,
        use_low_level_metrics: bool = True,
        num_workers: int = 4
    ) -> Dict[str, Any]:
        """
        Evaluates both low-level and high-level metrics in parallel using multiple workers.
        Falls back to sequential processing if parallel processing fails.
        
        Args:
            results: List of results to evaluate
            use_high_level_metrics: Whether to use high-level metrics
            use_low_level_metrics: Whether to use low-level metrics
            num_workers: Number of workers to use for parallel processing
            
        Returns:
            A dictionary containing all evaluation results
        """
        # Initialize metrics dictionary
        metrics = {
            "low_level_scores": {
                "chart_type": 0.0,
                "data_mapping": 0.0,
                "encoding": 0.0,
                "interaction": 0.0,
                "config": 0.0,
                "transform": 0.0,
                "overall": 0.0
            },
            "high_level_scores": {
                "chart_types": 0.0,
                "layout": 0.0,
                "text_content": 0.0,
                "data": 0.0,
                "style": 0.0,
                "clarity": 0.0,
                "overall": 0.0
            },
            "low_level_overall_score": 0.0,
            "high_level_overall_score": 0.0,
            "combined_overall_score": 0.0
        }
        
        wrong_results = []
        correct_results = []
        
        # Create result ID to original result mapping
        result_map = {i: result for i, result in enumerate(results)}
        
        # Add ID to each result for tracking
        for i, result in enumerate(results):
            result['id'] = i
        
        try:
            # Try parallel processing first
            _log("Attempting parallel evaluation with ProcessPoolExecutor")
            
            # Use ProcessPoolExecutor for parallel processing
            with ProcessPoolExecutor(max_workers=num_workers) as executor:
                # Submit all tasks
                futures = []
                for result in results:
                    try:
                        future = executor.submit(process_single_result, result, use_high_level_metrics, use_low_level_metrics)
                        futures.append(future)
                    except Exception as e:
                        _log(f"Error submitting task for result {result.get('id')}: {str(e)}")
                        # Add to wrong results if we can't even submit the task
                        result['log'] = f"Error submitting task: {str(e)}"
                        wrong_results.append(result)
                
                # Process results as they complete
                for future in tqdm(futures, total=len(futures)):
                    try:
                        processed = future.result(timeout=300)  # 5分钟超时
                        result_id = processed['result_id']
                        
                        # Get the original result using the ID
                        if result_id is not None and result_id in result_map:
                            original_result = result_map[result_id]
                            # Add log message to original result
                            original_result['log'] = processed['log']
                            
                            if processed['status'] == 'wrong':
                                wrong_results.append(original_result)
                            else:
                                correct_results.append(original_result)
                            
                            # Update metrics
                            self._update_metrics(metrics, processed, use_high_level_metrics, use_low_level_metrics, len(results))
                    except concurrent.futures.TimeoutError:
                        _log(f"Task timed out after 300 seconds")
                        # 将该任务标记为失败并继续处理其他任务
                    except Exception as e:
                        _log(f"Error processing future: {str(e)}\n{traceback.format_exc()}")
                        # 继续处理其他任务
        
        except Exception as e:
            _log(f"Parallel evaluation failed: {str(e)}. Falling back to sequential evaluation.")
            
            # Fall back to sequential processing
            return self._evaluate_sequential_fallback(results, use_high_level_metrics, use_low_level_metrics)
        
        self.metrics = metrics
        return metrics, wrong_results, correct_results
    
    def _update_metrics(self, metrics, processed, use_high_level_metrics, use_low_level_metrics, total_results):
        """Helper method to update metrics from processed results"""
        try:
            low_level_scores = processed.get('low_level_scores', {})
            high_level_scores = processed.get('high_level_scores', {})
            low_level_overall_score = processed.get('low_level_overall_score', 0.0)
            high_level_overall_score = processed.get('high_level_overall_score', 0.0)
            
            if use_low_level_metrics and low_level_scores:
                metrics['low_level_overall_score'] += low_level_overall_score / total_results
                for key, value in low_level_scores.items():
                    if key in metrics['low_level_scores']:
                        metrics['low_level_scores'][key] += value / total_results
            
            if use_high_level_metrics and high_level_scores:
                metrics['high_level_overall_score'] += high_level_overall_score / total_results
                for key, value in high_level_scores.items():
                    if key in metrics['high_level_scores']:
                        metrics['high_level_scores'][key] += value / total_results
            
            if use_low_level_metrics and use_high_level_metrics:
                metrics['combined_overall_score'] += (low_level_overall_score + high_level_overall_score) / 2.0 / total_results
            elif use_low_level_metrics:
                metrics['combined_overall_score'] += low_level_overall_score / total_results
            elif use_high_level_metrics:
                metrics['combined_overall_score'] += high_level_overall_score / total_results
        except Exception as e:
            _log(f"Error updating metrics: {str(e)}")

    def _evaluate_sequential_fallback(self, results, use_high_level_metrics, use_low_level_metrics):
        """Fallback to sequential evaluation if parallel processing fails"""
        _log("Using sequential evaluation fallback")
        # Call the regular evaluate method which processes sequentially
        return self.evaluate(results, use_high_level_metrics, use_low_level_metrics)

# 修改主函数部分，添加异常处理
if __name__ == "__main__":
    label = """import sqlite3
import pandas as pd
import altair as alt

conn = sqlite3.connect('database/customer_deliveries.sqlite')

query = '''
SELECT
    date(date_became_customer) AS year,
    COUNT(customer_id) AS num_customers
FROM Customers
GROUP BY
    year
ORDER BY
    year;
'''

df = pd.read_sql_query(query, conn)
df['cumulative_customers'] = df['num_customers'].cumsum()
conn.close()

line = alt.Chart(df).mark_line().encode(
    x=alt.X('year:O', axis=alt.Axis(labelAngle=-45)),
    y=alt.Y('cumulative_customers', scale=alt.Scale(type="log"), title='Cumulative Number of Customers')
)

points = alt.Chart(df).mark_circle(size=60).encode(
    x=alt.X('year:O', axis=alt.Axis(labelAngle=-45)),
    y=alt.Y('cumulative_customers', scale=alt.Scale(type="log"), title='Cumulative Number of Customers'),
    tooltip=['year', 'cumulative_customers']
)

chart1 = (line + points).properties(
    title='Customer Growth Over Time (Logarithmic Scale)'
)

chart1"""

    prediction = """import sqlite3
import pandas as pd
import altair as alt

conn = sqlite3.connect('database/customer_deliveries.sqlite')

query = '''
SELECT
    date(date_became_customer) AS year,
    COUNT(customer_id) AS num_customers
FROM Customers
GROUP BY
    year
ORDER BY
    year;
'''

df = pd.read_sql_query(query, conn)

df['cumulative_customers'] = df['num_customers'].cumsum()

conn.close()

chart2 = alt.Chart(df).mark_line(point=True).encode(
    x=alt.X('year:O', axis=alt.Axis(labelAngle=-45, title='Year')),
    y=alt.Y('cumulative_customers', scale=alt.Scale(type="log"), title='Cumulative Customers'),
    tooltip=['year', 'cumulative_customers']
).properties(
    title='Customer Growth Over Time (Logarithmic Scale)'
)

chart2"""

    try:
        metrics = VisualizationMetrics()
        result, wrong_results, correct_results = metrics.evaluate([{'prediction': prediction, 'label': label}], use_high_level_metrics=False, use_low_level_metrics=True)
        print(result)
    except Exception as e:
        print(f"Error in evaluation: {str(e)}")
        import traceback
        print(traceback.format_exc())